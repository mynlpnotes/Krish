# Which activation function to use when

* If we use sigmoid/tanh in hidden layer then we get vanishing gradient problem
* We use relu and its variants in hidden layer
*   In output we use sigmoid OR softmax

    <figure><img src="../../.gitbook/assets/image (4) (1).png" alt=""><figcaption></figcaption></figure>
